version: '3'

services:
  ollama:
    image: ollama/ollama
    networks:
      - no-internet
      - internet
    ports:
      - 11434:11434
    volumes:
      - ./:/root/.ollama
    environment:
      - OLLAMA_NUM_PARALLEL=5
      - OLLAMA_KEEP_ALIVE=120m

  
  init-stablediffusion:
    build:
      context: .
      dockerfile: Dockerfile.automatic1111
    entrypoint: bash -c "aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/ckpt/juggernaut-xl/resolve/main/juggernautXL_version2.safetensors -d /content/stable-diffusion-webui/models/Stable-diffusion -o juggernautXL_version2.safetensors"
    networks:
      - internet
  automatic1111:
    build:
      context: .
      dockerfile: Dockerfile.automatic1111
    # Komment out if you want to run all components on GPUs instead of CPU
    command: ["--use-cpu", "all"]
    restart: unless-stopped
    depends_on:
      - init-stablediffusion
    networks:
      - no-internet
    environment:
      - PYTHONUNBUFFERED=1
    ports:
      - 7860:7860
    volumes:
      - ./stable-diffusion:/content/stable-diffusion-webui/models/Stable-diffusion/
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    networks:
      - no-internet
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
      - AUTOMATIC1111_BASE_URL=http://stable-diffusion-webui:7860
    ports:
      - 8080:8080
  anythingllm:
    image: mintplexlabs/anythingllm
    container_name: anythingllm
    networks:
      - no-internet
      - internet
    ports:
    - "3001:3001"
    environment:
    # Adjust for your environment
      - STORAGE_DIR=/app/server/storage
      - JWT_SECRET="8VYqaqdPipSuJgFkYyf8hCowv3kygj8GpzEGjJX4RkShZ9xyAZyRcjVKhiP4b8oVbL37yWUsfbu"
      - LLM_PROVIDER=ollama
      - OLLAMA_BASE_PATH=http://ollama:11434
      - OLLAMA_MODEL_PREF=llama3.1
      - OLLAMA_MODEL_TOKEN_LIMIT=4096
      - EMBEDDING_ENGINE=ollama
      - EMBEDDING_BASE_PATH=http://ollama:11434
      - EMBEDDING_MODEL_PREF=nomic-embed-text:latest
      - EMBEDDING_MODEL_MAX_CHUNK_LENGTH=1192
      - VECTOR_DB=lancedb
      - WHISPER_PROVIDER=local
      - TTS_PROVIDER=native
      - PASSWORDMINCHAR=8
      - AGENT_SERPER_DEV_KEY="SERPER DEV API KEY"
      - AGENT_SERPLY_API_KEY="Serply.io API KEY"
      - VITE_API_BASE="http://<YOUR_REACHABLE_IP_ADDRESS>:3001/api"
      - DISABLE_TELEMETRY="true"
    volumes:
      - ./anythingllm:/app/server/storage
    restart: always

networks:
  no-internet:
    driver: bridge
    internal: true
  internet:
    driver: bridge
